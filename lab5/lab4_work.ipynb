{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## lab4\n",
    "\n",
    "### 实验目标\n",
    "\n",
    "在 lab 3 中，我们实现了 GraphSAGE（Hamilton 等人，2017）层。在这个 lab4 中，我们将实现一个更强大的层：GAT（Veličković等人，2018）。然后，我们将在 CORA 数据集上运行我们的模型。\n",
    "\n",
    "### 实验说明\n",
    "\n",
    "1. 需要大家完成的任务是加粗且带有得分的题目，如 `问题 i：XXXXXXX（15分）`\n",
    "2. 做完实验后，请举手通知助教检查实验代码以及问题的输出结果，以便给同学们进行打分\n",
    "3. 如果大家有疑问尽量在实验课的前60分钟提出，后30分钟主要用于检查同学们的实验结果，可能时间没那么充裕\n",
    "4. 本次实验没有评分的实验任务，所有任务都会提供参考答案。同学们只需要按照参考答案的实现或者自行思考实现，将控制台输出给助教展示供助教打分即可。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 实验过程\n",
    "\n",
    "（1）安装实验所需的依赖包"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 如果大家电脑里面有支持CUDA的GPU\n",
    "# pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
    "# pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
    "# pip install torch-geometric\n",
    "# pip install -q git+https://github.com/snap-stanford/deepsnap.git\n",
    "\n",
    "# 如果大家想用CPU来跑\n",
    "# 请参考文章：https://blog.csdn.net/dream__1/article/details/122433061\n",
    "\n",
    "# 验证是否安装成功，print(torch_geometric.__version__)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "（2）GNN Stack Module\n",
    "\n",
    "以下是通用 GNN 堆栈的实现，我们可以插入任何 GNN 层，例如 GraphSage、GAT 等。该模块已提供。在本次实验中实现的 GAT 层将作为 GNNStack 模块的组件运行。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_scatter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import softmax\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class GNNStack(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, args, emb=False):\n",
    "        super(GNNStack, self).__init__()\n",
    "        conv_model = self.build_conv_model(args.model_type)\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(conv_model(input_dim, hidden_dim))\n",
    "        assert (args.num_layers >= 1), 'Number of layers is not >=1'\n",
    "        for l in range(args.num_layers - 1):\n",
    "            self.convs.append(conv_model(args.heads * hidden_dim, hidden_dim))\n",
    "\n",
    "        # post-message-passing\n",
    "        self.post_mp = nn.Sequential(\n",
    "            nn.Linear(args.heads * hidden_dim, hidden_dim), nn.Dropout(args.dropout),\n",
    "            nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        self.dropout = args.dropout\n",
    "        self.num_layers = args.num_layers\n",
    "\n",
    "        self.emb = emb\n",
    "\n",
    "    def build_conv_model(self, model_type):\n",
    "        if model_type == 'GAT':\n",
    "            return GAT\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout)\n",
    "\n",
    "        x = self.post_mp(x)\n",
    "\n",
    "        if self.emb == True:\n",
    "            return x\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def loss(self, pred, label):\n",
    "        return F.nll_loss(pred, label)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "（3）创建`Message Passing Layer`\n",
    "\n",
    "> 本节中我们需要了解`Pytorch` 消息传递层的实现细节，从而构建自己的图神经网络模型。为此，我们将使用并实现 定义 `PyG `消息传递层所需的 3 个关键函数： `forward` ， `message` ，和 `aggregate` 。\n",
    "\n",
    "**什么是消息传递？**\n",
    "\n",
    "我们关注与单个中心节点 x 相关的单轮消息传递。在消息传递之前， x 与特征向量$x^{l-1}$相关联，消息传递的目的是更新这个特征向量$ x^l $。\n",
    "\n",
    " 为此，需要执行以下步骤：\n",
    "\n",
    "1. 每个相邻节点 v 通过边 (x,v) 传递其当前消息 $v^{l−1}$\n",
    "2. 对于节点 x ，我们聚合相邻节点的所有消息（例如通过求和或平均值）\n",
    "3. 通过例如应用线性和非线性变换来转换聚合信息。\n",
    "\n",
    "总的来说，消息传递过程被应用于我们的图中的每个节点 u ，u 通过作为上述步骤 1-3 中描述的中心节点 x 来更新其嵌入。\n",
    "\n",
    "现在，我们将此过程扩展到单层消息传递，消息传递层的任务是更新图中每个节点的当前特征表示或嵌入，通过在图中传播和转换信息来实现。总的来说，消息传递层的一般范式是：1）预处理 -> 2）消息传递/传播 -> 3）后处理。\n",
    "\n",
    "具体的`forward` 函数处理节点特征/嵌入的预处理和后处理，并通过调用 `propagate` 函数来启动消息传递。`propagate` 函数封装了消息传递过程！它通过调用三个重要的函数来实现：1) `message` ，2) `aggregate` ，和 3) `update` 。我们的实现与这个略有不同（我们不会显式地实现 `update` ，而是在 `forward` 函数中放置更新节点嵌入的逻辑）。更具体地说，在信息传递之后，我们可以进一步转换由 `propagate` 输出的节点嵌入。因此， `forward` 的输出正好是经过一个 `GNN` 层后的节点嵌入。\n",
    "\n",
    "> 了解上述函数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def propagate(edge_index, x=(x_i, x_j), extra=(extra_i, extra_j), size=size):\n",
    "    # edge_index 传递给 forward 函数并捕获图的边结构。\n",
    "    # x = (x_i, x_j) 表示将要用于消息传递的节点特征。为了解释为什么传递元组 (x_i, x_j) ，我们首先看看我们的边是如何表示的。对于每条边 (i,j) ∈ E，我们可以区分 i 为源节点（x_central）和 j 为相邻节点（x_neighbor）。\n",
    "    # 以上面的消息传递为例，对于中心节点 u ，我们将聚合并转换与节点 v 关联的所有消息，即 (u,v) ∈ E（即 v ∈ Nei_u  ）。因此我们可以看到，下标 _i 和 _j 允许我们具体区分与中心节点（即接收消息信息的节点）和相邻节点（即传递消息的节点）相关的特征。\n",
    "    # 还有一种理解方式：根据视角的不同，节点 x 可以作为一个中心节点或一个邻近节点。实际上，在无向图中，我们存储了边的两个方向（即 (i,j) 和 (j,i)）。从中心节点的角度来看x_i，x 正在收集邻近信息以更新其嵌入。从邻近节点的角度来看x_j，x 正在沿着连接到不同中心节点的边传递其消息信息。\n",
    "    # extra=(extra_i, extra_j) 代表我们可以与每个节点关联的除其当前特征嵌入之外的信息。实际上，我们可以包含我们想要的任何形式的额外参数 param=(param_i, param_j) 。再次强调，使用 _i 和 _j 进行索引使我们能够区分中心和邻近节点。\n",
    "\n",
    "    # 函数 propagate 的输出是消息传递过程后的节点嵌入矩阵，形状为  [N,d]  。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def message(x_j, ...):\n",
    "    # message 函数由 propagate 调用，并从相邻节点 j 到中心节点 i 为每条边 (i,j)  在 edge_index 中构建消息。此函数可以接受最初传递给 propagate 的任何参数。此外，我们还可以通过在变量名后附加 _i 或 _j 来再次区分中心节点和相邻节点，例如 x_i 和 x_j 。更具体地看变量，我们有：\n",
    "    # x_j 代表所有相邻节点通过各自的边传递消息的特征嵌入矩阵（即所有节点 j 对于边  (i,j) ∈ E），因此其形状为  [|E|,d]\n",
    "\n",
    "    # message 函数的输出是一个邻接节点嵌入矩阵，准备进行聚合，其形状为  [|E|,d]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def aggregate(self, inputs, index, dim_size = None):\n",
    "    #  aggregate 函数用来聚合来自邻接节点的消息\n",
    "    # inputs 代表从邻接节点传递的消息矩阵（即 message 函数的输出）\n",
    "    # index 与 inputs 具有相同的形状，并告诉我们每个行消息 j 在 inputs 矩阵中对应的中心节点。因此， index 告诉我们为每个中心节点聚合哪些行消息。\n",
    "\n",
    "    # aggregate 的输出形状为 [N,d]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**（4）问题1：实现 GAT （100分）**\n",
    "\n",
    "注意力机制已成为许多基于序列的任务（如机器翻译和学习句子表示）中的最先进技术。基于注意力机制的优点之一是它们能够专注于输入中最相关的部分来做出决策。在这个问题中，我们将看到如何通过使用图注意力网络（GATs）（Veličković等人，2018）来利用注意力机制对图结构数据进行节点分类。\n",
    "\n",
    "图注意力网络的基本构建模块是图注意力层，它是一种聚合函数的变体。设 N 为节点数， F 为每个节点的特征向量的维度。每个图注意力层的输入是一组节点特征：$\\mathbf{h} = \\{\\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N}$\\}, $\\overrightarrow{h_i} \\in R^F$。每个图注意力层的输出是一组新的节点特征，可能具有新的维度$F'$: $\\mathbf{h'} = \\{\\overrightarrow{h_1'}, \\overrightarrow{h_2'}, \\dots, \\overrightarrow{h_N'}\\}$,  $\\overrightarrow{h_i'} \\in \\mathbb{R}^{F'}$。\n",
    "\n",
    "我们将描述如何对每个图注意力层执行此转换\n",
    "\n",
    "首先，对每个节点应用由权重矩阵 $\\mathbf{W} \\in \\mathbb{R}^{F' \\times F}$ 参数化的共享线性转换。\n",
    "\n",
    "接下来对节点进行自注意力计算。我们使用共享的注意力函数 a:\n",
    "$$\n",
    "\\begin{equation}\n",
    "a : \\mathbb{R}^{F'} \\times \\mathbb{R}^{F'} \\rightarrow \\mathbb{R}.\n",
    "\\end{equation}\n",
    "$$\n",
    "然后计算节点 j 的特征对节点 i 重要性 attention 系数:\n",
    "$$\n",
    "\\begin{equation}\n",
    "e_{ij} = a(\\mathbf{W_l}\\overrightarrow{h_i}, \\mathbf{W_r} \\overrightarrow{h_j})\n",
    "\\end{equation}\n",
    "$$\n",
    "自注意力机制的最一般形式允许每个节点关注所有其他节点，这会丢失所有结构信息。为了在注意力机制中利用图结构，我们使用掩码注意力。在掩码注意力中，我们只为节点 $j \\in \\mathcal{N}_i$  计算注意力系数 $e_{ij}$ ，其中 $\\mathcal{N}_i$ 是图中节点 i 的某个邻域。\n",
    "\n",
    "为了方便在不同节点间比较系数，我们使用 softmax 函数对 j 中的系数进行归一化：\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\alpha_{ij} = \\text{softmax}_j(e_{ij}) = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})}\n",
    "\\end{equation}\n",
    "$$\n",
    "我们的注意力机制 a 将是一个单层前馈神经网络，由权重向量 $\\overrightarrow{a_l} \\in \\mathbb{R}^{F'}$ 和  $\\overrightarrow{a_r} \\in \\mathbb{R}^{F'}$ 参数化，随后接一个 Leaky ReLU 非线性（负输入斜率为 0.2）。令 $\\cdot^T$ 表示转置， || 表示拼接。我们注意力机制计算出的系数可以表示为：\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\alpha_{ij} = \\frac{\\exp\\Big(\\text{LeakyReLU}\\Big(\\overrightarrow{a_l}^T \\mathbf{W_l} \\overrightarrow{h_i} + \\overrightarrow{a_r}^T\\mathbf{W_r}\\overrightarrow{h_j}\\Big)\\Big)}{\\sum_{k\\in \\mathcal{N}_i} \\exp\\Big(\\text{LeakyReLU}\\Big(\\overrightarrow{a_l}^T \\mathbf{W_l} \\overrightarrow{h_i} + \\overrightarrow{a_r}^T\\mathbf{W_r}\\overrightarrow{h_k}\\Big)\\Big)}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "对于以下问题，我们记 `alpha_l` = $\\alpha_l = [...,\\overrightarrow{a_l}^T \\mathbf{W_l} \\overrightarrow{h_i},...] \\in \\mathcal{R}^n$ 、 `alpha_r` = $\\alpha_r = [..., \\overrightarrow{a_r}^T \\mathbf{W_r} \\overrightarrow{h_j}, ...] \\in \\mathcal{R}^n$ 。\n",
    "\n",
    "在 GAT 的每一层中，在计算完该层的注意力系数后，可以通过对邻居消息进行加权求和来计算聚合函数，其中权重由 $\\alpha_{ij}$ 指定。\n",
    "\n",
    "现在，我们使用归一化的注意力系数来计算与之对应的特征线性组合。这些聚合特征将作为每个节点的最终输出特征。\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "h_i' = \\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij} \\mathbf{W_r} \\overrightarrow{h_j}.\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "> Multi-Head Attention 多头注意力\n",
    "\n",
    "为了稳定自注意力机制的学习过程，我们使用多头注意力机制。为此，我们使用 K 个独立的注意力机制，或称为“头”，按照上述方程计算输出特征。然后，我们将这些输出特征表示连接起来：\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\overrightarrow{h_i}' = ||_{k=1}^K \\Big(\\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij}^{(k)} \\mathbf{W_r}^{(k)} \\overrightarrow{h_j}\\Big)\n",
    "\\end{equation}\n",
    "$$\n",
    "其中 $||$ 表示拼接, $\\alpha_{ij}^{(k)}$ 是由第 $k$ 个 注意力机制  $(a^k)$ 计算出的归一化注意力系数， $\\mathbf{W}^{(k)}$ 是相应的输入线性变换的权重矩阵。请注意，对于此设置 $\\mathbf{h'} \\in \\mathbb{R}^{KF'}$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GAT(MessagePassing):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, heads = 2,\n",
    "                 negative_slope = 0.2, dropout = 0., **kwargs):\n",
    "        super(GAT, self).__init__(node_dim=0, **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lin_l = None\n",
    "        self.lin_r = None\n",
    "        self.att_l = None\n",
    "        self.att_r = None\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: code here!\n",
    "        # 定义self.lin_l（线性层）。\n",
    "        # 我们使用的是多头注意力，所以请注意线性层的维度。\n",
    "\n",
    "        ############################################################################\n",
    "\n",
    "        self.lin_r = self.lin_l\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: code here!\n",
    "        # (1) 初始化注意力参数(self.att_l, self.att_r)\n",
    "        # (2) 注意多头注意力的场景\n",
    "        # (3) 使用 nn.Parameter 代替 nn.Linear\n",
    "\n",
    "        ############################################################################\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.lin_l.weight)\n",
    "        nn.init.xavier_uniform_(self.lin_r.weight)\n",
    "        nn.init.xavier_uniform_(self.att_l)\n",
    "        nn.init.xavier_uniform_(self.att_r)\n",
    "\n",
    "    def forward(self, x, edge_index, size = None):\n",
    "\n",
    "        H, C = self.heads, self.out_channels\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: code here!\n",
    "        # 执行消息传递以及任何前处理和后处理（更新规则）。\n",
    "        # 1. 首先对节点嵌入应用线性变换，并将其分成多个头。我们对源节点和目标节点使用相同的表示法，但应用不同的线性权重（W_l 和 W_r）\n",
    "        # 2. 计算中心节点（alpha_l）和邻居节点（alpha_r）的 alpha 向量。\n",
    "        # 3. 调用 propagate 函数进行信息传递。\n",
    "        # 3.1 记住将 alpha = (alpha_l, alpha_r) 作为参数传递。\n",
    "        # 3.2 更多信息请参考：https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html。\n",
    "        # 4. 将输出转换回 [N, H * C] 的形状。\n",
    "\n",
    "\n",
    "        ############################################################################\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def message(self, x_j, alpha_j, alpha_i, index, ptr, size_i):\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: code here!\n",
    "        # 1. 使用 alpha_i 和 alpha_j 计算最终注意力权重，并应用 Leaky Relu 激活函数。\n",
    "        # 2. 计算所有节点邻居节点的 softmax。使用 torch_geometric.utils.softmax 代替 Pytorch 中的 softmax。\n",
    "        # 3. 对注意力权重 (alpha) 应用 dropout。\n",
    "        # 4. 将嵌入和注意力权重相乘。作为正确性检查，输出的维度应为 [E, H, C] 。\n",
    "        # ptr （LongTensor类型，可选参数）： 如果给定，则根据 CSR 表示中的排序输入计算 softmax。你可以简单地将它传递给 softmax。\n",
    "\n",
    "\n",
    "        ############################################################################\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def aggregate(self, inputs, index, dim_size = None):\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: code here!\n",
    "\n",
    "        ############################################################################\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "（5）Building Optimizers 构建优化器\n",
    "\n",
    "这个函数已经实现。为了方便助教评分，请使用默认的 Adam 优化器，但同学们也可以自由地尝试其他类型的优化器看能否取得更好的效果。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_optimizer(args, params):\n",
    "    weight_decay = args.weight_decay\n",
    "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
    "    if args.opt == 'adam':\n",
    "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'sgd':\n",
    "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
    "    elif args.opt == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'adagrad':\n",
    "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    if args.opt_scheduler == 'none':\n",
    "        return None, optimizer\n",
    "    elif args.opt_scheduler == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
    "    elif args.opt_scheduler == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
    "    return scheduler, optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "（6）训练和测试"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(dataset, args):\n",
    "    print(\"Node task. test set size:\", np.sum(dataset[0]['train_mask'].numpy()))\n",
    "    test_loader = loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    # build model\n",
    "    model = GNNStack(dataset.num_node_features, args.hidden_dim, dataset.num_classes,\n",
    "                     args)\n",
    "    scheduler, opt = build_optimizer(args, model.parameters())\n",
    "\n",
    "    # train\n",
    "    losses = []\n",
    "    test_accs = []\n",
    "    for epoch in range(args.epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch in loader:\n",
    "            opt.zero_grad()\n",
    "            pred = model(batch)\n",
    "            label = batch.y\n",
    "            pred = pred[batch.train_mask]\n",
    "            label = label[batch.train_mask]\n",
    "            loss = model.loss(pred, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "        total_loss /= len(loader.dataset)\n",
    "        losses.append(total_loss)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            test_acc = test(test_loader, model)\n",
    "            test_accs.append(test_acc)\n",
    "        else:\n",
    "            test_accs.append(test_accs[-1])\n",
    "    return test_accs, losses\n",
    "\n",
    "\n",
    "def test(loader, model, is_validation=True):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        with torch.no_grad():\n",
    "            # max(dim=1) returns values, indices tuple; only need indices\n",
    "            pred = model(data).max(dim=1)[1]\n",
    "            label = data.y\n",
    "\n",
    "        mask = data.val_mask if is_validation else data.test_mask\n",
    "        # node classification: only evaluate on nodes in test set\n",
    "        pred = pred[mask]\n",
    "        label = data.y[mask]\n",
    "\n",
    "        correct += pred.eq(label).sum().item()\n",
    "\n",
    "    total = 0\n",
    "    for data in loader.dataset:\n",
    "        total += torch.sum(data.val_mask if is_validation else data.test_mask).item()\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "（7）输出结果\n",
    "\n",
    "我们将使用 CORA 数据集进行节点级别的分类，为了便于助教评分，请不要修改默认参数。但是，同学们也可以自由地尝试不同的配置"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for args in [\n",
    "    {'model_type': 'GAT', 'dataset': 'cora', 'num_layers': 2, 'heads': 1, 'batch_size': 32, 'hidden_dim': 32,\n",
    "     'dropout': 0.5, 'epochs': 500, 'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0, 'weight_decay': 5e-3,\n",
    "     'lr': 0.01},\n",
    "]:\n",
    "    args = objectview(args)\n",
    "    for model in ['GAT']:\n",
    "        args.model_type = model\n",
    "\n",
    "        # Match the dimension.\n",
    "        if model == 'GAT':\n",
    "            args.heads = 2\n",
    "        else:\n",
    "            args.heads = 1\n",
    "\n",
    "        if args.dataset == 'cora':\n",
    "            dataset = Planetoid(root='./dataset/data1', name='Cora')\n",
    "        else:\n",
    "            raise NotImplementedError(\"Unknown dataset\")\n",
    "        test_accs, losses = train(dataset, args)\n",
    "\n",
    "        print(\"Maximum accuracy: {0}\".format(max(test_accs)))\n",
    "        print(\"Minimum loss: {0}\".format(min(losses)))\n",
    "\n",
    "        plt.title(dataset.name)\n",
    "        plt.plot(losses, label=\"training loss\" + \" - \" + args.model_type)\n",
    "        plt.plot(test_accs, label=\"test accuracy\" + \" - \" + args.model_type)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
